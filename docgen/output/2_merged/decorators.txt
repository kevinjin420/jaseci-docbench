# Decorators and Annotations

# Decorators and Annotations

## Node/Walker Decorators

-   `@on_entry`: Executes when a walker enters a node or edge.
    ```python
    class TaskCreator(Walker):
        # ...
        @on_entry
        def create(self, here) -> None:
            """Entry point - creates task."""
            # ...
    ```
-   `@on_exit`: Executes when a walker exits a node or edge.

## Semantic Decorators

-   `@sem(doc, fields)`: Attaches semantic metadata for AI/LLM integration.
    -   **Usage (Jac):** `@sem("doc", {"field": "desc"})`
    -   **Usage (Python):**
        ```python
        from jaclang import JacRuntimeInterface as Jac
        from dataclasses import dataclass
        from byllm.lib import Model, by

        @Jac.sem('<Person Semstring>', {
            'name' : '<name semstring>',
            'age' : '<age semstring>',
            'ssn' : "<ssn semstring>"
            }
        )
        @dataclass
        class Person:
            name: str
            age: int
            ssn: int
        ```
        ```python
        from jaclang import JacRuntimeInterface as Jac
        from dataclasses import dataclass
        from byllm.lib import Model, by
        llm =  Model(model_name="gemini/gemini-2.0-flash")

        @Jac.sem('', {  'name': 'Full name of the person',
                        'dob': 'Date of Birth',
                        'ssn': 'Last four digits of the Social Security Number of a person'
                        })
        @dataclass
        class Person():
            name: str
            dob: str
            ssn: str

        @by(llm)
        def check_eligibility(person: Person, service_type: str) -> bool: ...
            """Calculate eligibility for various services based on person's data."""
        ```

## LLM Integration Decorator

-   `@by(model)`: Decorator for functions powered by Language Model (LLM) inference.
    -   **Module:** `byllm.lib`
    -   **Usage:**
        ```python
        import jaclang
        from dataclasses import dataclass
        from byllm.lib import Model, Image, by
        from enum import Enum

        llm = Model(model_name="gpt-4o")

        @dataclass
        class Person:
            full_name: str
            description: str
            year_of_birth: int

        @by(llm)
        def get_person_info(img: Image) -> Person: ...

        @by(llm)
        def translate_to(language: str, phrase: str) -> str: ...

        class Personality(Enum):
            INTROVERT
            EXTROVERT
            AMBIVERT

        @by(model=llm)
        def get_personality(name: str) -> Personality: ...
        ```
    -   **With Hyper-parameters:**
        ```python
        import jaclang
        from byllm.lib import Model, by

        llm = Model(model_name="gpt-4o")

        @by(llm(temperature=0.3))
        def generate_joke() -> str: ...
        ```

## Plugin Hooks

-   `@hookimpl`: Decorator for defining plugin hook implementations.
    -   **`call_llm` Hook Signature:**
        ```python
        @hookimpl
        def call_llm(
            model: Model,
            caller: Callable,
            args: dict[str | int, object]
        ) -> object:
            """
            Called when Jaclang executes a 'by llm()' statement.

            Args:
                model: The Model instance with configuration
                caller: The function being called with LLM
                args: Arguments passed to the function

            Returns:
                The result that should be returned to the Jaclang program
            """
        ```
    -   **Example Implementation:**
        ```python
        @staticmethod
        @hookimpl
        def call_llm(
            model: Model, caller: Callable, args: dict[str | int, object]
        ) -> object:
            """Custom LLM call implementation."""
            print(f"Custom plugin intercepted call to: {caller.__name__}")
            print(f"Arguments: {args}")

            # Option 1: Modify the call and delegate to the original model
            result = model.invoke(caller, args)

            # Option 2: Implement completely custom logic
            # result = your_custom_llm_logic(caller, args)

            print(f"Result: {result}")
            return result
        ```