# Imports and Modules

## Imports

### Python Imports (`import:py`)
Used to import Python modules and objects into Jac.

```jac
import:py from byllm.lib { Model };
import:py from byllm.llm { BaseLLM };
import:py from openai { OpenAI };
```

### Jac Imports (`import`)
Used to import Jac modules and objects.

```jac
import from byllm.llm { BaseLLM };
import from openai { OpenAI };
```

## Custom Language Model (LLM) Integration

### Python `BaseLLM` Implementation
To create a custom LLM, inherit from `byllm.llm.BaseLLM` and implement `model_call_no_stream` and `model_call_with_stream`.

```python
from byllm.llm import BaseLLM
from openai import OpenAI

class MyOpenAIModel(BaseLLM):
    def __init__(self, model_name: str, **kwargs: object) -> None:
        """Initialize the MockLLM connector."""
        super().__init__(model_name, **kwargs)

    def model_call_no_stream(self, params):
        client = OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(**params)
        return response

    def model_call_with_stream(self, params):
        client = OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(stream=True, **params)
        return response
```

### Jac `BaseLLM` Implementation
Jac objects can also implement custom LLMs by inheriting from `BaseLLM`.

```jac
import from byllm.llm { BaseLLM };
import from openai { OpenAI };

obj MyOpenAIModel(BaseLLM){
    has model_name: str;
    has config: dict = {};

    def post_init() {
        super().__init__(model_name=self.model_name, **kwargs);
    }

    def model_call_no_stream(params: dict) {
        client = OpenAI(api_key=self.api_key);
        response = client.chat.completions.create(**params);
        return response;
    }

    def model_call_with_stream(params: dict) {
        client = OpenAI(api_key=self.api_key);
        response = client.chat.completions.create(stream=True, **params);
        return response;
    }
}
```

## `byllm.lib.Model`

### Usage with LiteLLM Proxy
The `Model` class from `byllm.lib` can be used to interact with LLMs, including those exposed via a LiteLLM proxy.

```python
from byllm.lib import Model

llm = Model(
    model_name="gpt-4o",                # The model name to be used
    api_key="your_litellm_api_key",     # LiteLLM proxy server key
    proxy_url="http://localhost:8000",  # URL of the LiteLLM proxy server
)
```