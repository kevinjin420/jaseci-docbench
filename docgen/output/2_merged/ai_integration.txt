# AI Integration

# AI Integration

Jac is designed to integrate AI directly into the programming model, simplifying the development of AI-powered applications.

## byLLM

`byLLM` is a Jaclang plugin and Python package that provides AI functionality, primarily for integrating Large Language Models (LLMs). It automates prompt engineering and enables seamless model integration.

### Core Concepts

-   **`by llm()`**: A language-level construct in Jac that automatically generates optimized prompts and integrates with LLMs.
-   **Semantic Strings (`str "..."`)**: Introduced with the `sem` keyword, these attach natural language descriptions to code elements (functions, classes, parameters) for LLM interpretation.
-   **Agentic AI Workflows**: Jac's Object-Spatial Programming (OSP) and `by llm` facilitate the creation of interacting agents that collaborate and share memory.
-   **Streaming with ReAct Tool Calling**: Supports real-time streaming for ReAct methods with tools, providing structured tool calling and streaming responses.

### Installation

```bash
pip install byllm
```

### Usage in Jac

#### Model Declaration

```jac
import from byllm.lib { Model, Image, Video }

glob llm = Model(model_name="gpt-4o");
```

#### Function Overriding with LLM

Use `by llm()` to define functions whose implementation is provided by an LLM.

```jac
def get_person_info(img: Image) -> Person by llm();
```

#### Hyper-parameters

Pass hyper-parameters directly to the `by llm()` call.

```jac
def generate_joke() -> str by llm(temperature=0.3);
```

#### Semantic Strings (`sem`)

Attach natural language descriptions to types or attributes.

```jac
'Personality of the Person'
enum Personality {
   INTROVERT,
   EXTROVERT
}

sem Personality.INTROVERT = 'Person who is shy and reticent';
sem Personality.EXTROVERT = 'Person who is outgoing and socially confident';

obj Person {
    has full_name: str,
        yod: int,
        personality: Personality;
}

sem Person.description = "Short biography"
```

#### Multimodal Input

`byLLM` supports `Image` and `Video` types for multimodal LLM interactions.

```jac
import from byllm.lib { Model, Image }

glob llm = Model(model_name="gpt-4o");

obj Person {
    has full_name: str,
        yod: int,
        personality: Personality;
}

def get_person_info(img: Image) -> Person by llm();

with entry {
    image = Image("photo.jpg");
    person_obj = get_person_info(image);
    print(person_obj);
}
```

```jac
import from byllm.lib { Model, Video }

glob llm = Model(model_name="gpt-4o");

def explain_the_video(video: Video) -> str by llm();

with entry {
    video_file_path = "SampleVideo_1280x720_2mb.mp4";
    target_fps = 1
    video = Video(path=video_file_path, fps=target_fps);
    print(explain_the_video(video));
}
```

### Usage in Python

`byLLM` can be used directly in Python or by importing Jac modules into Python.

#### Direct Python Integration

Import `Model`, `Image`, and `by` from `byllm.lib`.

```python
import jaclang
from dataclasses import dataclass
from byllm.lib import Model, Image, by

llm = Model(model_name="gpt-4o")

@dataclass
class Person:
    full_name: str
    description: str
    year_of_birth: int

@by(llm)
def get_person_info(img: Image) -> Person: ...

img = Image("https://bricknellschool.co.uk/wp-content/uploads/2024/10/einstein3.webp")

person = get_person_info(img)
print(f"Name: {person.full_name}, Description: {person.description}, Year of Birth: {person.year_of_birth}")
```

#### Python Hyper-parameters

```python
import jaclang
from byllm.lib import Model, by

llm = Model(model_name="gpt-4o")

@by(llm(temperature=0.3))
def generate_joke() -> str: ...
```

#### Python Functions as Tools

Python functions can be passed as tools to the LLM.

```python
import jaclang
from byllm.lib import Model
llm = Model(model_name="gpt-4o")

def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

@by(llm(tools=[get_weather]))
def answer_question(question: str) -> str: ...
```

#### Python Semantic Enrichment (`@Jac.sem`)

The Python implementation of `sem` is a work-in-progress and may change.

```python
from jaclang import JacRuntimeInterface as Jac
from byllm.lib import Model, by

@Jac.sem('<Person Semstring>', {
    'name' : '<name semstring>',
    'age' : '<age semstring>',
    'ssn' : "<ssn semstring>"
    }
)
@dataclass
class Person:
    name: str
    age: int
    ssn: int
```

#### Importing Jac Modules into Python (Recommended)

Jac modules with `byLLM` features can be seamlessly imported into Python.

**`main.py`**:
```python
import jaclang
from .ai import Image, Person, get_person_info

img = Image("https://bricknellschool.co.uk/wp-content/uploads/2024/10/einstein3.webp")

person = get_person_info(img)
print(f"Name: {person.full_name}, Description: {person.description}, Year of Birth: {person.year_of_birth}")
```

**`ai.jac`**:
```jac
import from byllm.lib {Model, Image}

glob llm = Model(model_name="gpt-4o");

obj Person{
    has full_name: str;
    has description: str;
    has year_of_birth: int;
}

sem Person.description = "Short biography"

def get_person_info(img: Image) -> Person by llm();
```

### LiteLLM Proxy Integration

`byLLM` models can connect to a LiteLLM proxy server.

```python
from byllm.lib import Model

llm = Model(
    model_name="gpt-4o",                # The model name to be used
    api_key="your_litellm_api_key",     # LiteLLM proxy server key (not OpenAI API key)
    proxy_url="http://localhost:8000",  # URL of the LiteLLM proxy server
)
```

### Custom Model Declaration

Custom model interfaces can be defined by inheriting from `BaseLLM` from `byllm.lib`.

## Standard Library

### `file` Module

-   `open(path: str) -> File`

### `os` Module

(No specific functions provided in input, but `os` is a standard Python module.)

### `json` Module

-   `dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False) -> str`
-   `loads(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None) -> Any`

### `logging` Module

(No specific functions provided in input, but `logging` is a standard Python module.)

## Jac Syntax & Features

### Imports

-   `import:py`: Used for importing Python modules.
-   `import from <module> { <symbols> }`: Used for importing specific symbols from Jac modules.

### Spawning

-   `spawn`: Used to create new nodes or edges. Can be used on lists/groups.

### Type Annotations

-   `has x: type`: Explicit type annotation for variables or object attributes.

### Reference Operator

-   `&`: Reference operator.

### Node Properties

-   `.edges`: Property of a node, typically representing its connected edges.

### Semantic String

-   `str "..."`: A string literal used for semantic annotations.

### Asynchronous Programming

-   `async`: Defines an asynchronous function.
-   `await`: Pauses execution until an awaitable completes.

### Testing/Validation

-   `assert`: Used for asserting conditions, typically in testing or validation.

### Persistence

-   `save`: Saves data or state.
-   `load`: Loads previously saved data or state.

### Agent Loops

Jac supports agentic AI workflows, often implemented using `while` loops with `by llm()`.

```jac
# Example of an agent loop pattern
walker agent_loop {
    while (condition) {
        // Agent performs actions, potentially using LLM
        response = some_function() by llm();
        // Process response, update state
    }
}
```

## AI Integration with byLLM

byLLM is an AI integration framework for the Jaseci ecosystem, implementing the Meaning Typed Programming (MTP) paradigm. It embeds prompt engineering directly into code semantics, making AI interactions more natural and maintainable. While primarily designed for Jac, byLLM also provides a Python library interface.

### Core Concepts

*   **Meaning Typed Programming (MTP)**: AI automatically classifies and routes requests based on simple definitions, embedding prompt engineering into code semantics.
*   **Model Context Protocol (MCP)**: Build modular, reusable AI tools.
*   **Multimodal AI**: Work with text, images, and videos within a single application.

### `by llm()` Abstraction

The `by llm()` keyword enables functions to process inputs of any type and generate contextually appropriate outputs of the specified type, auto-generating prompts and ensuring strict adherence to the return type.

#### Jac Usage

```jac
import from byllm.lib {Model}

glob llm = Model(model_name="gemini/gemini-2.0-flash");

enum Personality {
    INTROVERT,
    EXTROVERT,
    AMBIVERT
}

def get_personality(name: str) -> Personality by llm();

with entry {
    name = "Albert Einstein";
    result = get_personality(name);
    print(f"{result} personality detected for {name}");
}
```

#### Python Usage

```python
import jaclang
from byllm.lib import Model, by
from enum import Enum

llm = Model(model_name="gemini/gemini-2.0-flash")

class Personality(Enum):
    INTROVERT
    EXTROVERT
    AMBIVERT

@by(model=llm)
def get_personality(name: str) -> Personality: ...

name = "Albert Einstein"
result = get_personality(name)
print(f"{result} personality detected for {name}")
```

### Semantic Control

Docstrings and semantic strings (`sem`) enrich code semantics for LLM integration.

#### Jac Semantic Strings

```jac
import from byllm.lib { Model }
glob llm = Model(model_name="gemini/gemini-2.0-flash");

"""Represents the personal record of a person"""
obj Person {
    has name: str;
    has dob: str;
    has ssn: str;
}

sem Person.name = "Full name of the person";
sem Person.dob = "Date of Birth";
sem Person.ssn = "Last four digits of the Social Security Number of a person";

"""Calculate eligibility for various services based on person's data."""
def check_eligibility(person: Person, service_type: str) -> bool by llm();
```

#### Python Semantic Strings

```python
from jaclang import JacRuntimeInterface as Jac
from dataclasses import dataclass
from byllm.lib import Model, by
llm =  Model(model_name="gemini/gemini-2.0-flash")

@Jac.sem('', {  'name': 'Full name of the person',
                'dob': 'Date of Birth',
                'ssn': 'Last four digits of the Social Security Number of a person'
                })
@dataclass
class Person():
    name: str
    dob: str
    ssn: str

@by(llm)
def check_eligibility(person: Person, service_type: str) -> bool: ...
    """Calculate eligibility for various services based on person's data."""
```

### Supported Models

byLLM integrates with various LLM providers (OpenAI, Anthropic, Google, etc.) through [LiteLLM](https://litellm.ai/).

```jac
import from byllm.lib {Model}

glob llm = Model(model_name = "gpt-4o") # OpenAI
glob llm = Model(model_name = "gemini/gemini-2.0-flash") # Gemini
```

### Custom Model Class

To use a self-hosted language model, a custom API, or any service not supported by LiteLLM, create a custom Model class inheriting from `BaseLLM`.

#### Python Custom Model

```python
from byllm.llm import BaseLLM
from openai import OpenAI

class MyOpenAIModel(BaseLLM):
    def __init__(self, model_name: str, **kwargs: object) -> None:
        """Initialize the MockLLM connector."""
        super().__init__(model_name, **kwargs)

    def model_call_no_stream(self, params):
        client = OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(**params)
        return response

    def model_call_with_stream(self, params):
        client = OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(stream=True, **params)
        return response
```

#### Jac Custom Model

```jac
import from byllm.llm { BaseLLM }
import from openai { OpenAI }

obj  MyOpenAIModel(BaseLLM){
    has model_name: str;
    has config: dict = {};

    def post_init() {
        super().__init__(model_name=self.model_name, **kwargs);
    }

    def model_call_no_stream(params: dict) {
        client = OpenAI(api_key=self.api_key);
        response = client.chat.completions.create(**params);
        return response;
    }

    def model_call_with_stream(params: dict) {
        client = OpenAI(api_key=self.api_key);
        response = client.chat.completions.create(stream=True, **params);
        return response;
    }
}
```

Initialize your custom model:

```jac
glob llm = MyOpenAIModel(model_name="gpt-4o");
```

### byLLM Plugin System

byLLM uses a plugin architecture based on [Pluggy](https://pluggy.readthedocs.io/) to extend or modify how `by llm()` handles LLM calls.

#### Plugin Functionality

Plugins can:
*   Implement custom LLM providers.
*   Add preprocessing/postprocessing logic.
*   Implement caching mechanisms.
*   Add logging or monitoring.
*   Create mock implementations for testing.

#### Plugin Architecture

1.  **Hook Specifications**: Define the interface plugins must implement.
2.  **Hook Implementations**: Your plugin code implementing the hooks.
3.  **Plugin Registration**: How plugins are discovered and loaded.

When `by llm()` is used, the runtime system looks for registered plugins that implement the `call_llm` hook.

### Environment Variables

Set API keys for LLM providers and other services:

```bash
export GEMINI_API_KEY="your-api-key-here"
export OPENAI_API_KEY=<your-openai-key>
export SERPER_API_KEY=<your-serper-key>
```

```
## Plugin Development

Plugins extend Jaclang's LLM capabilities. They are Python packages that register entry points.

### Plugin Package Structure

```
my-byllm-plugin/
├── pyproject.toml
├── README.md
└── my_byllm_plugin/
    ├── __init__.py
    └── plugin.py
```

### Plugin Class Definition

Plugins implement the `call_llm` hook.

```python
"""Custom byLLM Plugin."""

from typing import Callable, Any
import hashlib
import json
import time

from jaclang.runtimelib.runtime import hookimpl
from byllm.llm import Model


class MybyllmRuntime:
    """Custom byLLM Plugin Implementation."""

    @staticmethod
    @hookimpl
    def call_llm(
        model: Model, caller: Callable, args: dict[str | int, object]
    ) -> object:
        """Custom LLM call implementation."""
        print(f"Custom plugin intercepted call to: {caller.__name__}")
        print(f"Arguments: {args}")

        # Option 1: Modify the call and delegate to the original model
        result = model.invoke(caller, args)

        # Option 2: Implement completely custom logic
        # result = your_custom_llm_logic(caller, args)

        print(f"Result: {result}")
        return result
```

### Plugin Configuration (`pyproject.toml`)

Register the plugin using entry points under `[tool.poetry.plugins."jac"]`.

```toml
[tool.poetry]
name = "my-byllm-plugin"
version = "0.1.0"
description = "My custom byLLM plugin"
authors = ["Your Name <your.email@example.com>"]

[tool.poetry.dependencies]
python = "^3.11"
byllm = "*"
jaclang = "*"

[tool.poetry.plugins."jac"]
my-byllm-plugin = "my_byllm_plugin.plugin:MybyllmRuntime"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
```

### Plugin Installation

```bash
pip install -e .
```

### Plugin Hook Reference

#### `call_llm` Hook

The primary hook for intercepting LLM calls.

```python
@hookimpl
def call_llm(
    model: Model,
    caller: Callable,
    args: dict[str | int, object]
) -> object:
    """
    Called when Jaclang executes a 'by llm()' statement.

    Args:
        model: The Model instance with configuration.
        caller: The function being called with LLM.
        args: Arguments passed to the function.

    Returns:
        The result that should be returned to the Jaclang program.
    """
```

### Plugin Examples

#### Caching Plugin

```python
class CachingbyllmRuntime:
    """Plugin that caches LLM responses."""

    _cache: dict[str, Any] = {}

    @staticmethod
    @hookimpl
    def call_llm(
        model: Model, caller: Callable, args: dict[str | int, object]
    ) -> object:
        """Cache LLM responses."""
        cache_key = hashlib.md5(
            json.dumps({
                "function": caller.__name__,
                "args": str(args),
                "model": model.model_name
            }, sort_keys=True).encode()
        ).hexdigest()

        if cache_key in CachingbyllmRuntime._cache:
            print(f"Cache hit for {caller.__name__}")
            return CachingbyllmRuntime._cache[cache_key]

        result = model.invoke(caller, args)
        CachingbyllmRuntime._cache[cache_key] = result
        print(f"Cached result for {caller.__name__}")
        return result
```

#### Logging Plugin

```python
class LoggingbyllmRuntime:
    """Plugin that logs all LLM calls."""

    @staticmethod
    @hookimpl
    def call_llm(
        model: Model, caller: Callable, args: dict[str | int, object]
    ) -> object:
        """Log LLM calls with timing information."""
        start_time = time.time()
        print(f"[LLM CALL] Starting: {caller.__name__}")
        print(f"[LLM CALL] Model: {model.model_name}")
        print(f"[LLM CALL] Args: {args}")

        try:
            result = model.invoke(caller, args)
            duration = time.time() - start_time
            print(f"[LLM CALL] Completed: {caller.__name__} in {duration:.2f}s")
            print(f"[LLM CALL] Result: {result}")
            return result
        except Exception as e:
            duration = time.time() - start_time
            print(f"[LLM CALL] Failed: {caller.__name__} after {duration:.2f}s")
            print(f"[LLM CALL] Error: {e}")
            raise
```

#### Custom Model Provider Plugin

```python
class CustomProviderRuntime:
    """Plugin that implements a custom model provider."""

    @staticmethod
    @hookimpl
    def call_llm(
        model: Model, caller: Callable, args: dict[str | int, object]
    ) -> object:
        """Handle custom model providers."""
        if model.model_name.startswith("custom://"):
            return CustomProviderRuntime._handle_custom_model(
                model, caller, args
            )
        return model.invoke(caller, args)

    @staticmethod
    def _handle_custom_model(
        model: Model, caller: Callable, args: dict[str | int, object]
    ) -> object:
        """Implement custom model logic."""
        model_type = model.model_name.replace("custom://", "")
        if model_type == "echo":
            return f"Echo: {list(args.values())[0]}"
        elif model_type == "random":
            import random
            responses = ["Yes", "No", "Maybe", "I don't know"]
            return random.choice(responses)
        else:
            raise ValueError(f"Unknown custom model: {model_type}")
```

### Plugin Best Practices

-   **Handle Errors Gracefully**: Use `try-except` blocks.
    ```python
    @hookimpl
    def call_llm(model: Model, caller: Callable, args: dict[str | int, object]) -> object:
        try:
            return model.invoke(caller, args)
        except Exception as e:
            print(f"LLM call failed: {e}")
            return "Error: Unable to process request"
    ```
-   **Preserve Original Functionality**: Delegate to `model.invoke(caller, args)` unless completely replacing.
    ```python
    @hookimpl
    def call_llm(model: Model, caller: Callable, args: dict[str | int, object]) -> object:
        # Your pre-processing logic
        result = model.invoke(caller, args)  # Delegate to original
        # Your post-processing logic
        return result
    ```
-   **Use Configuration**: Load settings from environment or files.
    ```python
    class ConfigurableRuntime:
        def __init__(self):
            self.config = self._load_config()

        def _load_config(self):
            # Load from environment, file, etc.
            return {"enabled": True, "log_level": "INFO"}

        @hookimpl
        def call_llm(self, model: Model, caller: Callable, args: dict[str | int, object]) -> object:
            if not self.config["enabled"]:
                return model.invoke(caller, args)
            # Plugin logic implementation
    ```
-   **Testing**: Create comprehensive tests.
    ```python
    import pytest
    from byllm.llm import Model
    from my_byllm_plugin.plugin import MybyllmRuntime

    def test_plugin():
        runtime = MybyllmRuntime()
        model = Model("mockllm", outputs=["test response"])

        def test_function(x: str) -> str:
            """Test function."""
            pass

        result = runtime.call_llm(model, test_function, {"x": "test input"})
        assert result == "test response"
    ```

### Plugin Discovery and Loading

Plugins are automatically discovered and loaded when installed as Python packages with the `"jac"` entry point in `pyproject.toml`. Discovery happens via `plugin_manager.load_setuptools_entrypoints("jac")`.

### Debugging Plugins

-   **Enable Debug Logging**: `export JAC_DEBUG=1`
-   **Verify Registration**:
    ```python
    from jaclang.runtimelib.runtime import plugin_manager
    for plugin in plugin_manager.get_plugins():
        print(f"Loaded plugin: {plugin}")
    ```

### Common Plugin Pitfalls

-   Missing `@hookimpl` decorator.
-   Incorrect entry point name (must be `"jac"`).
-   Wrong hook signature for `call_llm`.
-   Forgetting to delegate to `model.invoke()` when necessary.

## AI Integration

### Model Configuration

Configure the LLM for AI operations.

```jac
import:py from byllm.lib {Model}

glob llm = Model(model_name="gpt-4o");
```

### AI-Powered Functions

Define functions that leverage LLM capabilities using `by llm()`.

```jac
def make_player() -> Person by llm();

def make_random_npc() -> Person by llm();

can get_answer(question: str) -> str by llm();
```

### Conversational AI Agents

Agents can maintain state, reason, and act using tools.

```jac
def chat_with_player(player: Person, npc: Person, chat_history: list[Chat]) -> Chat
    by llm(method="ReAct", tools=[make_transaction]);
```
-   **Maintains State**: Uses `chat_history` for context.
-   **Reasons**: Processes conversation context (e.g., using `ReAct` method).
-   **Acts**: Can use tools (e.g., `make_transaction`).
-   **Persists Context**: Builds understanding across interactions.

### Agent Loops

Agents can operate within loops, making decisions based on LLM output.

```jac
# Example of an agent loop (conceptual)
walker agent_loop {
    has current_state: str = "initial";
    while (current_state != "finished") {
        current_state = by llm(prompt="Decide next state based on current_state: " + current_state)();
        // Perform actions based on current_state
    }
}
```

### Intelligent Routing

AI-powered classification for routing requests to specialized handlers.
-   **Nodes**: `TaskHandling`, `EmailHandling`, `GeneralChat`
-   **Walker**: `task_manager` for routing and coordination.

### Task Management

-   **Add Tasks**: Create tasks with dates and times.
-   **Task Summarization**: Get summaries of scheduled tasks.
-   **Smart Extraction**: Automatically extracts task details from natural language.
-   **Smart Scheduling**: Conflict detection, optimal time suggestions, calendar integration, timezone handling.
-   **Task Dependencies**: Prerequisite tracking, automatic prioritization, progress monitoring, bottleneck identification.

### Email Writing

-   Generate professional emails.
-   Context-aware content creation.
-   **Email Template System**: Dynamic email generation.
-   **Email Context Integration**: Incorporates task info, meeting summaries, status updates, reminder emails.

### General Chat

-   Ask questions and get intelligent responses.
-   Advice on productivity and time management.
-   General AI assistance.

### Learning and Adaptation

-   **User Pattern Recognition**: Learns from user preferences.
-   **Improved Routing**: Enhances intent classification.
-   **Personalized Responses**: Adapts communication style.
-   **Context Memory**: Maintains long-term conversation context.

## Standard Library

### `file` Module

-   `open(path: str) -> File`

### `os` Module

(No specific functions provided in input, but generally for operating system interaction.)

### `json` Module

-   `dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False) -> str`
-   `loads(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None) -> Any`

### `logging` Module

(No specific functions provided in input, but generally for logging events.)

## Syntax & Operators

-   `import:py from <module> {<name>}`: Python imports.
-   `spawn <node_or_edge>`: Create new nodes or edges.
-   `spawn <list_of_nodes_or_edges>`: Create multiple nodes or edges.
-   `has x: type`: Explicit type annotations.
-   `&`: Reference operator.
-   `.edges`: Node property to access connected edges.
-   `str "..."`: Semantic String.
-   `async`, `await`: Asynchronous programming keywords.
-   `assert`: Testing and validation.
-   `save`, `load`: Persistence operations.
```

```jac
with entry {
    player = make_player();
    npc = make_random_npc();

    person_record[player.name] = player;
    person_record[npc.name] = npc;

    history = [];

    while True {
        chat = chat_with_player(player, npc, history);
        history.append(chat);

        for p in [player, npc] {
            print(p.name, ":  $", p.money);
            for i in p.inventory {
                print("  ", i.name, ":  $", i.price);
            }
        }

        print("\n[[npc]] >> ", chat.message);
        inp = input("\n[[Player input]] >> ");
        history.append(Chat(person=player.name, message=inp));
    }
}
```

## AI Integration

### AI Functions (Stateless)
AI-integrated functions that operate without persistent state.
- `make_player()`: Generates a player character.
- `make_random_npc()`: Generates a random NPC.
These are AI-powered utilities, not agents.

### AI Agents (Stateful)
AI systems that maintain persistent state across interactions.
- `chat_with_player(player, npc, history)`: Retains conversation context through the `history` parameter. Builds understanding over multiple turns and can reference previous interactions.

### Tool Integration
AI agents can access application functions through tools.
- The `chat_with_player` AI agent can call `make_transaction`.
- The AI extracts parameters from natural language.
- Tool results are incorporated into responses.

### State Management
AI agents maintain state through:
- Structured data objects (e.g., `Person`, `InventoryItem`).
- Conversation history (e.g., `Chat` objects).
- Global registries (e.g., `person_record`).

### `by llm()`
Syntax for integrating LLMs to generate dynamic content, such as game levels.

## Standard Library

### `file` Module
- `open(path: str) -> File`

### `json` Module
- `dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False) -> str`
- `loads(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None) -> Any`

## Syntax & Operators

### Imports
- `import:py from <module> {<name1>, <name2>}`: Imports specific names from a Python module.

### Spawning
- `spawn`: Used for creating new nodes or edges. Can be used on lists/groups.

### Type Annotations
- `has x: type`: Explicit type annotation for variables or fields.

### Reference Operator
- `&`: Reference operator.

### Node Properties
- `.edges`: Property of a node, representing its connected edges.

### Semantic String
- `str "..."`: Denotes a semantic string.

### Asynchronous Operations
- `async`: Defines an asynchronous function.
- `await`: Pauses execution until an asynchronous operation completes.

### Testing/Validation
- `assert <condition>`: Used for testing and validation, raises an error if the condition is false.

### Persistence
- `save`: Persists data to storage.
- `load`: Loads data from storage.
```