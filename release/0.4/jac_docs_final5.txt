Jac Technical Reference Abilities are methods in node and walker objects. They can be callable or visit-dependent. Callable Abilities are invoked directly. Example: node testnode { has value: int; def callable { print(f"===== Callable on {self}"); } }. Visit-Dependent Abilities trigger automatically when a walker interacts with a node or during a walker's traversal. self refers to the object (node or walker) where the ability is defined. here refers to the visiting walker within a node's ability, or the visited node within a walker's ability. visitor refers to the walker interacting with a node/edge. Node Abilities Triggered by Walker Events: Nodes define abilities that trigger when a walker enters or exits them, or when a specific type of walker enters or exits. Node abilities execute before walker abilities. Example: node Node { has val: str; can entry1 with entry { print(f"{self.val}-2"); } can entry2 with Walker entry { print(f"{self.val}-3"); } can exit1 with Walker exit { print(f"{self.val}-4"); } can exit2 with exit { print(f"{self.val}-5"); } }. Example: node testnode { has value: int; can logentry with entry { print(f">>> Some Walker entered the node: ", self); } can logtestwalkerentry with testwalker entry { print(f">>> {here} entered the node {self}"); here.callable(); } can logtestwalkerexit with testwalker exit { print(f"<<< {here} exited the node {self}"); } can logexit with exit { print(f"<<< Some Walker exited the node {self}"); } }. Walker Abilities Triggered by Traversal Events: Walkers define abilities that trigger at various points in their lifecycle or during traversal. can <abilityname> with entry: Triggered once when the walker is spawned. can <abilityname> with <NodeType> entry: Triggered each time the walker visits a node of the specified type. can <abilityname> with <NodeType> exit: Triggered each time the walker exits a node of the specified type. can <abilityname> with exit: Triggered once when the walker finishes its traversal. Example: walker Greeter { has greetingcount: int = 0; can start with `root entry { print("Starting journey!"); visit [-->]; } can greet with Person entry { print(f"Hello, {here.name}!"); self.greetingcount += 1; visit [-->]; } }. Example: walker Walker { can entry1 with entry { print("walker entry"); } can entry2 with `root entry { print("walker enter to root"); visit [-->]; } can entry3 with Node entry { print(f"{here.val}-1"); } can exit1 with Node exit { print(f"{here.val}-6"); } can exit2 with exit { print("walker exit"); } }. Special Entry Points for Walkers: Walkers define abilities that trigger based on their starting point. Entry through root: walker walkername { can walkerability with `root entry; }. Entry through a given node: walker walkername { can walkerability with specificnode entry; }. Entry through root or a given node: walker walkername { can walkerability with `root | specificnode entry; }. Agentic Patterns: Abilities are fundamental to creating agentic systems, where nodes act as agents responding to visitor events. Example: can execute with taskmanager entry { print("[TaskHandling Node Activated]"); response = self.routeandrun(visitor.curtask.task); print("â†’", response); report { "utterance": visitor.curtask.task, "response": response, "nodetype": self.class.name }; }. Example: can execute with `root entry { subtasks = self.plantasks(self.utterance); print("[Planned Subtasks]:", subtasks); nodemap = { RoutingNodes.TASKHANDLING: TaskHandling, RoutingNodes.EMAILHANDLING: EmailHandling, RoutingNodes.GENERALCHAT: GeneralChat }; for subtask in subtasks { nodetype = nodemap[subtask.agenttype]; routednode = [-->(`?nodetype)]; if not routednode { routednode = here ++> nodetype(); } self.curtask = subtask; visit routednode; } }. AI Agents Agent Loops: Agents maintain state and execute actions within a loop, processing conversation context and utilizing tools. Example: with entry { player = makeplayer(); npc = makerandomnpc(); personrecord[player.name] = player; personrecord[npc.name] = npc; history = []; while True { chat = chatwithplayer(player, npc, history); history.append(chat); for p in [player, npc] { print(p.name, ": $", p.money); for i in p.inventory { print(" ", i.name, ": $", i.price); } } print("\n[[npc]] >> ", chat.message); inp = input("\n[[Player input]] >> "); history.append(Chat(person=player.name, message=inp)); } }. Stateful Agents: AI agents maintain persistent state across interactions, such as conversation history. Example: def chatwithplayer(player: Person, npc: Person, chathistory: list[Chat]) -> Chat by llm(method="ReAct", tools=[maketransaction]);. This agent maintains state using chathistory, reasons using ReAct, acts using tools like maketransaction, and persists context. Tool Integration: AI agents access application functions through tools. The by llm() directive specifies the method and available tools. The AI extracts parameters from natural language, and tool results are incorporated into responses. AI Abilities by llm() Abstraction: The by llm() abstraction enables functions to process inputs of any type and generate contextually appropriate outputs of the specified type. It auto-generates a prompt and provides an output strictly adhering to the specified return type. Example Jac: import from byllm.lib {Model} glob llm = Model(modelname="gemini/gemini-2.0-flash"); def getpersonality(name: str) -> Personality by llm();. Example Python: import jaclang from byllm.lib import Model, by from enum import Enum llm = Model(modelname="gemini/gemini-2.0-flash") class Personality(Enum): INTROVERT EXTROVERT AMBIVERT @by(model=llm) def getpersonality(name: str) -> Personality: .... AI Agent Characteristics: AI agents maintain conversation state, reason using methods like ReAct, and act by using tools. They use chathistory for state, process context with ReAct, use tools, and build understanding across turns. Streaming with ReAct Tool Calling: Real-time streaming is supported for the ReAct method with tools. After tool execution, the LLM streams the final synthesized answer token-by-token. byLLM Feature Methods as Tools: byLLM supports adding class methods as tools for the LLM using tools=[ToolHolder.tool]. AI Integration Concepts by llm(): The by() keyword automatically generates optimized prompts for LLM integration, removing manual prompt engineering. It can be used as a Python library or natively in Jac. Agentic AI Workflows: Jac's graph-based semantics support agentic workflows, enabling interacting agents to collaborate, share memory, and act on dynamic context. Mean Typed Programming (MTP): MTP embeds prompt engineering into code semantics, making AI interactions natural and maintainable. It allows AI to classify and route user queries based on simple definitions. Model Context Protocol (MCP): MCP facilitates building modular, reusable AI tools. Semantic Strings (sem): sem strings attach natural language descriptions to code elements (functions, classes, parameters) for AI-powered code generation and execution. byllm Module byllm is a Jaclang plugin and Python package for AI functionality. Installation: pip install byllm. Python Integration: import jaclang from dataclasses import dataclass from byllm.lib import Model, Image, by llm = Model(modelname="gpt-4o") @dataclass class Person: fullname: str description: str yearofbirth: int @by(llm) def getpersoninfo(img: Image) -> Person: ... img = Image("https://bricknellschool.co.uk/wp-content/uploads/2024/10/einstein3.webp") person = getpersoninfo(img) print(f"Name: {person.fullname}, Description: {person.description}, Year of Birth: {person.yearofbirth}"). Jac Integration: import from byllm.lib {Model, Image} glob llm = Model(modelname="gpt-4o"); obj Person{ has fullname: str; has description: str; has yearofbirth: int; } sem Person.description = "Short biography" def getpersoninfo(img: Image) -> Person by llm();. byllm.lib.Model: Initializes an LLM model. Model(modelname: str, apikey: str = None, proxyurl: str = None, kwargs). modelname: The name of the LLM model (e.g., "gpt-4o", "gemini/gemini-2.0-flash"). apikey: API key for the LLM service. proxyurl: URL for a LiteLLM proxy server. kwargs: Additional model-specific parameters. Model Hyper-parameters: Control model behavior (e.g., temperature). Jac: glob llm = Model(modelname="gpt-4o"); def generatejoke() -> str by llm(temperature=0.3);. Python: from byllm.lib import Model, by llm = Model(modelname="gpt-4o") @by(llm(temperature=0.3)) def generatejoke() -> str: .... Using Python Functions as Tools: Python functions can be passed to byllm as tools for the LLM. from byllm.lib import Model, by llm = Model(modelname="gpt-4o") def getweather(city: str) -> str: return f"The weather in {city} is sunny." @by(llm(tools=[getweather])) def answerquestion(question: str) -> str: .... Semantic Enrichment with sem: Jac: obj Person { has name:str; has age:int; has ssn: int; } sem Person.ssn = "last four digits of the Social Security number". Python: from jaclang import JacRuntimeInterface as Jac from dataclasses import dataclass from byllm.lib import Model, by @Jac.sem('<Person Semstring>', { 'name' : '<name semstring>', 'age' : '<age semstring>', 'ssn' : "<ssn semstring>" }) @dataclass class Person: name: str age: int ssn: int. Custom Model Declaration: Custom model interfaces can be defined by inheriting from byllm.llm.BaseLLM. Python: from byllm.llm import BaseLLM from openai import OpenAI class MyOpenAIModel(BaseLLM): def init(self, modelname: str, kwargs: object) -> None: super().init(modelname, kwargs) def modelcallnostream(self, params): client = OpenAI(apikey=self.apikey) response = client.chat.completions.create(params) return response def modelcallwithstream(self, params): client = OpenAI(apikey=self.apikey) response = client.chat.completions.create(stream=True, params) return response. Jac: import from byllm.llm { BaseLLM } import from openai { OpenAI } obj MyOpenAIModel(BaseLLM){ has modelname: str; has config: dict = {}; def postinit() { super().init(modelname=self.modelname, kwargs); } def modelcallnostream(params: dict) { client = OpenAI(apikey=self.apikey); response = client.chat.completions.create(params); return response; } def modelcallwithstream(params: dict) { client = OpenAI(apikey=self.apikey); response = client.chat.completions.create(stream=True, **params); return response; } }. AI-Powered Applications Multimodal Chatbot: Jac supports building multimodal AI applications with Intelligent Routing (MTP), Document Processing (vector embeddings), Tool Servers (MCP tools), and AI Vision (images/videos). Code Generation: Jac builds AI-powered code generation tools with task planning and validation. Task Planning System analyzes requests, breaks down problems, creates dependency graphs, and prioritizes execution. Code Generation Pipeline involves request analysis, context gathering, code synthesis, and quality assurance. Validation Framework ensures code quality through syntax checking, logic analysis, best practice compliance, and security assessment. Jac Cloud Overview: Jac Cloud transforms Jac applications into production-ready API servers. It deploys Jac applications as RESTful APIs, WebSocket services, and scheduled tasks. It replaces jac run with jac serve for web service deployment and provides cloud-native features without additional code. Instant API Generation: Automatically converts Jac walkers into REST endpoints. Supports HTTP methods (GET, POST, PUT, DELETE). Built-in OpenAPI/Swagger documentation at /docs. Quick Start: Install Jac Cloud: pip install jac-cloud. Serve as an API: jac serve main.jac. Applications are available at http://localhost:8000 with API documentation at http://localhost/docs.